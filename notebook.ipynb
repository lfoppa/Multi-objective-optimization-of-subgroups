{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "import seaborn as sns\n",
    "import os\n",
    "from scipy.interpolate import interp1d\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_input(path, df, id_job, n_cutoffs, algo, dev, n_res, n_seeds, target_key):\n",
    "    \"\"\"\n",
    "    Creates the two input files necessary to run SGD using realkd:\n",
    "    i) a .json file with calculation details, named \"id_job.json\", and\n",
    "    ii) a .xarf file with the data set, named \"id_job.xarf\".\n",
    "    Arguments: path(str): path to the folder where the files \n",
    "                                   will be written\n",
    "               df(data frame): data set containing the values for the \n",
    "                               candidate descriptive parameters and for\n",
    "                               the target for all adsorption sites\n",
    "               id_job(str): job name\n",
    "               n_cutoffs(int): number of cutoffs to be used in k-Means\n",
    "                               clustering to generate the propositions\n",
    "               algo(str): SG search algorithm:\n",
    "                          PMM_SAMPLER \n",
    "                          EMM_SAMPLER\n",
    "                          EXCEPTIONAL_SUBGROUP_BESTFIRST_BRANCHANDBOUND\n",
    "                          \n",
    "                          PMM_SAMPLER uses (std(SG)-std(P))/std(P) as utility function\n",
    "                          whereas EMM_SAMPLER/EXCEPTIONAL_SUBGROUP_BESTFIRST_BRANCHANDBOUND \n",
    "                          use the function specified in dev\n",
    "               \n",
    "               dev(str): deviation measure when using EMM_SAMPLER: \n",
    "                         cumulative_jensen_shannon_divergence\n",
    "                         normalized_positive_mean_shift\n",
    "                         normalized_negative_mean_shift\n",
    "                         normalized_positive_median_shift\n",
    "                         normalized_negative_median_shift\n",
    "                         \n",
    "               n_res(int): number of results, i.e., number of top-ranked\n",
    "                           SGs to display\n",
    "               n_seeds(int): number of seeds to use for the SG search\n",
    "               target_key(str): label of the variable to be used as target quantity in SGD\n",
    "    \"\"\"\n",
    "    df.to_csv(path+'/'+id_job+'.csv')\n",
    "    with open(path+'/'+id_job+'.csv', 'r') as file_in:\n",
    "        data = file_in.read().splitlines(True)\n",
    "        \n",
    "    file_out = open(path+'/'+id_job+'.xarf', 'w')\n",
    "    file_out.write('@relation '+id_job+'\\n')\n",
    "    file_out.write('@attribute sites name\\n')\n",
    "    for variable in list(df.columns):\n",
    "        file_out.write('@attribute '+variable+' numeric\\n')\n",
    "    file_out.write(\"@data\\n\")\n",
    "    file_out.close()\n",
    "\n",
    "    with open(path+'/'+id_job+'.xarf', 'a') as file_out:\n",
    "        file_out.writelines(data[1:])\n",
    "        file_out.close()\n",
    "    \n",
    "    input_file = {}\n",
    "    input_file = {\"type\" : \"productWorkScheme\",\n",
    "                  \"id\" : id_job,\n",
    "                  \"workspaces\" : [ {\n",
    "                                \"type\" : \"workspaceFromXarf\",\n",
    "                                \"id\" : id_job,\n",
    "                                \"datafile\" : id_job+\".xarf\",\n",
    "                                \"propScheme\": {\"type\": \"standardPropScheme\",\n",
    "                                                \"defaultMetricRule\": {\"type\": \"kmeansPropRule\",\n",
    "                                                                       \"numberOfCutoffs\": n_cutoffs,\n",
    "                                                                       \"maxNumberOfIterations\": 1000}}} ],\n",
    "                    \"computations\" : [ {\n",
    "                                \"type\" : \"legacyComputation\",\n",
    "                                \"id\" : \"subgroup_analysis\",\n",
    "                                \"algorithm\" : algo,\n",
    "                                \"parameters\" : {\n",
    "                                    \"dev_measure\": dev,\n",
    "                                    \"attr_filter\" : \"[]\",\n",
    "                                    \"cov_weight\" : \"1.0\",\n",
    "                                    \"num_res\" : n_res,\n",
    "                                    \"num_seeds\" : n_seeds,\n",
    "                                    \"targets\" : \"[\"+target_key+\"]\"\n",
    "                                             }\n",
    "                  }],\n",
    "                  \"computationTimeLimit\" : 3600000\n",
    "                     }\n",
    "    with open(path+'/'+id_job+'.json','w') as outfile:\n",
    "        json.dump(input_file, outfile, indent=4)\n",
    "        \n",
    "\n",
    "def analyze(file_results):\n",
    "    \"\"\"\n",
    "    Extracts information about the identified SGs from the realkd output file into a data frame.\n",
    "    The values of relative SG size (coverage), utility function, quality function, the mean value of\n",
    "    the target property in the SG as well as the SG constraints are considered for each identified SG. \n",
    "    Argument: file_results: realkd's output json file\n",
    "    \"\"\"\n",
    "    list_coverages=[]\n",
    "    list_utility_function=[]\n",
    "    list_quality_function=[]\n",
    "    list_target_mean=[]\n",
    "    list_constraints=[]\n",
    "    \n",
    "    with open(file_results) as json_file:\n",
    "        data = json.load(json_file)\n",
    "        for index in range(len(data)):\n",
    "            coverage=data[index].get('measurements')[0].get('value')\n",
    "            utility_function=data[index].get('measurements')[1].get('value')\n",
    "            quality_function=coverage*utility_function\n",
    "            target_mean=data[index].get('descriptor').get('targetLocalModel').get('means')\n",
    "            list_attributes=data[index].get('descriptor').get('selector').get('attributes')\n",
    "            list_operators=[]\n",
    "            list_cutoffs=[]\n",
    "            constraints=[]\n",
    "            for i in list(range(0,len(list_attributes))):\n",
    "                list_operators.append(data[index].get('descriptor').get('selector').get('constraints')[i].get('type'))\n",
    "                list_cutoffs.append(round(data[index].get('descriptor').get('selector').get('constraints')[i].get('value'),4))\n",
    "\n",
    "            list_operators = [op.replace('lessOrEquals', '<=') for op in list_operators]\n",
    "            list_operators = [op.replace('greaterOrEquals', '>=') for op in list_operators]\n",
    "            list_operators = [op.replace('lessThan', '<') for op in list_operators]\n",
    "            list_operators = [op.replace('greaterThan', '>') for op in list_operators]\n",
    "    \n",
    "            for i in list(range(0,len(list_attributes))):\n",
    "                if i == 0:\n",
    "                    constraints=list_attributes[0]+list_operators[0]+str(list_cutoffs[0])\n",
    "                else:\n",
    "                    constraints=constraints+' & '+list_attributes[i]+list_operators[i]+str(list_cutoffs[i])\n",
    "            list_coverages.append(coverage)\n",
    "            list_utility_function.append(utility_function)\n",
    "            list_quality_function.append(quality_function)\n",
    "            list_target_mean.append(*target_mean)\n",
    "            list_constraints.append(constraints)\n",
    "            \n",
    "    df = pd.DataFrame(list(zip(list_coverages,\n",
    "                               list_utility_function,\n",
    "                               list_quality_function,\n",
    "                               list_target_mean,\n",
    "                               list_constraints)), \n",
    "                      columns =['coverage','utility','quality','target_mean','constraints'])\n",
    "    return(df)\n",
    "\n",
    "\n",
    "def get_pareto_frontier(Xs, Ys, maxX=True, maxY=True):\n",
    "    \"\"\"\n",
    "    Identifies the Pareto front.\n",
    "    Arguments: Xs (array): values of first objective\n",
    "               Ys (array): values of second objective\n",
    "    \"\"\"\n",
    "    sorted_list = sorted([[Xs[i], Ys[i], i] for i in range(len(Xs))], reverse=maxY)\n",
    "    pareto_front = [sorted_list[0]]\n",
    "    for pair in sorted_list[1:]:\n",
    "        if maxY:\n",
    "            if pair[1] >= pareto_front[-1][1]:\n",
    "                pareto_front.append(pair)\n",
    "        else:\n",
    "            if pair[1] <= pareto_front[-1][1]:\n",
    "                pareto_front.append(pair)\n",
    "    \n",
    "    return(pareto_front)\n",
    "\n",
    "def get_pareto_region(Xs, Ys, threshold, maxX=True, maxY=True):\n",
    "    \"\"\"\n",
    "    Identifies the Pareto region defined as the solutions at the Pareto front plus the solutions within \n",
    "    a threhold distance to the solutions of the Pareto front.\n",
    "    Arguments: Xs (array): values of first objective\n",
    "               Ys (array): values of second objective\n",
    "               threshold (float): threshold distance to the Pareto front\n",
    "    \"\"\"\n",
    "    sorted_list = sorted([[Xs[i], Ys[i], i] for i in range(len(Xs))], reverse=maxY)\n",
    "    pareto_front = [sorted_list[0]]\n",
    "    for pair in sorted_list[1:]:\n",
    "        if maxY:\n",
    "            if pair[1] >= pareto_front[-1][1]:\n",
    "                pareto_front.append(pair)\n",
    "        else:\n",
    "            if pair[1] <= pareto_front[-1][1]:\n",
    "                pareto_front.append(pair)\n",
    "    \n",
    "    pf_X = [pair[0] for pair in pareto_front]\n",
    "    pf_Y = [pair[1] for pair in pareto_front]\n",
    "    x_max=max(pf_X)\n",
    "    x_min=min(pf_X)\n",
    "    pf_f=interp1d(pf_X, pf_Y)\n",
    "    \n",
    "    extended_pf=[]\n",
    "    for i in np.arange(x_min,x_max,0.0001):\n",
    "        extended_pf.append([i,pf_f(i)])\n",
    "    pareto_region=[]\n",
    "    \n",
    "    for i in range(len(sorted_list)):\n",
    "        distances=[]\n",
    "        for j in range(len(extended_pf)):\n",
    "            d=((sorted_list[i][0]-extended_pf[j][0])**2 + (sorted_list[i][1]-extended_pf[j][1])**2)**0.5\n",
    "            distances.append(d)\n",
    "        distances_sorted=sorted(distances)\n",
    "        if distances_sorted[0] < threshold :\n",
    "            pareto_region.append(sorted_list[i])\n",
    "    return(pareto_region)\n",
    "\n",
    "def get_pareto_frontiers(Xs, Ys, n, maxX=True, maxY=True):\n",
    "    \"\"\"\n",
    "    Identifies successive Pareto fronts. The second pareto front is the Pareto front that would be obtained if the\n",
    "    solutions of the (first) Pareto front were excluded, and so on.\n",
    "    Arguments: Xs (array): values of first objective\n",
    "               Ys (array): values of second objective\n",
    "               n (int): number of successive Pareto fronts to be identified. \n",
    "    \"\"\"\n",
    "    data = [[Xs[i], Ys[i], i] for i in range(len(Xs))]  \n",
    "    pareto_fronts = []\n",
    "\n",
    "    for _ in range(n):\n",
    "        if not data:\n",
    "            break\n",
    "\n",
    "        pareto_front = []\n",
    "        remaining_data = []\n",
    "\n",
    "        for point in data:\n",
    "            x, y, _ = point\n",
    "            is_dominated = False\n",
    "            for other in data:\n",
    "                ox, oy, _ = other\n",
    "                if (maxX and ox > x or not maxX and ox < x) and (maxY and oy > y or not maxY and oy < y):\n",
    "                    is_dominated = True\n",
    "                    break \n",
    "\n",
    "            if not is_dominated:\n",
    "                pareto_front.append(point)\n",
    "            else:\n",
    "                remaining_data.append(point)\n",
    "\n",
    "        pareto_fronts.append(pareto_front)\n",
    "        data = remaining_data \n",
    "\n",
    "    return pareto_fronts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the two datasets utilized to train SGD\n",
    "#SAC indicates that the dataset contains Solid, Atomic, and Compositional features\n",
    "#AC indicates that the dataset contains only Atomic and Compositional features\n",
    "df_SAC=pd.read_csv('./data/dataset_SAC_features.csv').set_index('material')\n",
    "df_AC=pd.read_csv('./data/dataset_AC_features.csv').set_index('material')\n",
    "\n",
    "#SGD settings\n",
    "target_label=['bulk_modulus']\n",
    "n_clusters=10\n",
    "n_seeds=50000\n",
    "n_results=5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_job='positive_mean_shift_SAC'\n",
    "write_input('./', \n",
    "            df_SAC, \n",
    "            id_job, \n",
    "            n_clusters,\n",
    "            'EMM_SAMPLER',\n",
    "            'normalized_positive_mean_shift',\n",
    "            n_results, \n",
    "            n_seeds,\n",
    "            target_label[0])\n",
    "#the line below runs the realkd code, but it is commented since the output files used to obtain \n",
    "#the results described in the publication are provided\n",
    "#os.system('java -jar realkd-0.7.2-jar-with-dependencies.jar '+id_job+'.json') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_job='cJSD_SAC'\n",
    "write_input('./', \n",
    "            df_SAC, \n",
    "            id_job, \n",
    "            n_clusters,\n",
    "            'EMM_SAMPLER',\n",
    "            'cumulative_jensen_shannon_divergence',\n",
    "            n_results, \n",
    "            n_seeds,\n",
    "            target_label[0])\n",
    "#os.system('java -jar realkd-0.7.2-jar-with-dependencies.jar '+id_job+'.json') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_job='cJSD_AC'\n",
    "write_input('./', \n",
    "            df_AC, \n",
    "            id_job, \n",
    "            n_clusters,\n",
    "            'EMM_SAMPLER',\n",
    "            'cumulative_jensen_shannon_divergence',\n",
    "            n_results, \n",
    "            n_seeds,\n",
    "            target_label[0])\n",
    "#os.system('java -jar realkd-0.7.2-jar-with-dependencies.jar '+id_job+'.json') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analysis of SGD solutions and identification of Pareto front and Pareto region\n",
    "for id_job in ['positive_mean_shift_SAC', 'cJSD_SAC','cJSD_AC']:\n",
    "    file_results='./output/'+id_job+'/'+os.listdir('./output/'+id_job+'/')[0]+'/results/'+id_job+'_subgroup_analysis.json'\n",
    "    df_results=analyze(file_results)\n",
    "    df_results.to_csv('results_'+id_job+'.csv')\n",
    "        \n",
    "    pareto_front=get_pareto_frontier(df_results['coverage'], \n",
    "                                         df_results['utility'], \n",
    "                                         maxX=True, maxY=True)\n",
    "    print('The Pareto front for the SG search', id_job, 'contains', len(pareto_front), 'SGs.')\n",
    "    pf_indices=[pair[2] for pair in pareto_front]\n",
    "    df_pf=df_results.loc[pf_indices, :]\n",
    "    df_pf.sort_values(by=['coverage'],inplace=True)\n",
    "    df_pf.to_csv('pf_'+id_job+'.csv')\n",
    "        \n",
    "    threshold=0.01\n",
    "    pareto_region=get_pareto_region(df_results['coverage'], \n",
    "                                        df_results['utility'], \n",
    "                                        threshold, maxX=True, maxY=True)\n",
    "    print('The Pareto region for the SG search', id_job, 'contains', len(pareto_region), 'SGs.')\n",
    "    pr_indices=[pair[2] for pair in pareto_region]\n",
    "    df_pr=df_results.loc[pr_indices, :]\n",
    "    df_pr.sort_values(by=['coverage'],inplace=True)\n",
    "    df_pr.to_csv('pr_'+id_job+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualization of the results\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "#the SG index (with respect to the Pareto region, sorted according to increasing coverage) \n",
    "#can be chosen to explore the different SGs identified by the multi-objective approach\n",
    "SG_index=10\n",
    "\n",
    "for id_job in ['positive_mean_shift_SAC', 'cJSD_SAC','cJSD_AC']:\n",
    "    fig,(ax1,ax2,ax3) = plt.subplots(1,3, constrained_layout=True, figsize=(10,4))\n",
    "    df_results=pd.read_csv('results_'+id_job+'.csv')\n",
    "    df_pf=pd.read_csv('pf_'+id_job+'.csv')\n",
    "    df_pr=pd.read_csv('pr_'+id_job+'.csv')\n",
    "        \n",
    "    d=20\n",
    "    c=['grey','darkorange','blue', 'dodgerblue','red']\n",
    "    ax2.set_title('Results for SG search '+id_job)  \n",
    "    ax1.scatter(df_results['coverage'],df_results['utility'],c=c[0],s=d)\n",
    "    ax1.scatter(df_pr['coverage'],df_pr['utility'],c=c[3],s=d)\n",
    "    ax1.scatter(df_pf['coverage'],df_pf['utility'],c=c[2],s=d)\n",
    "    ax1.plot(df_pf['coverage'],df_pf['utility'],c=c[2])\n",
    "\n",
    "    ax1.set_ylabel('$u(SG,\\widetilde{P})$')\n",
    "    ax1.set_xlabel('$\\\\frac{s(SG)}{s(\\widetilde{P})}$')\n",
    "    ax1.set_xlim(0,1)\n",
    "    ax1.set_ylim(0,1)\n",
    "    \n",
    "    x=np.arange(0,1,0.01).tolist()\n",
    "    Q_function=[df_results['coverage'][0]*df_results['utility'][0]/x[i] for i in range(len(x))]\n",
    "    ax1.plot(x,Q_function,c=c[1],linestyle='dashed')\n",
    "    ax1.scatter(df_results['coverage'][0],df_results['utility'][0],c=c[1],s=d)\n",
    "    ax1.scatter(df_pr['coverage'][SG_index],df_pr['utility'][SG_index],c=c[4],s=d)\n",
    " \n",
    "    ax1.text(0.1,0.9,'Pareto front',color=c[2],fontsize=12)\n",
    "    ax1.text(0.1,0.85,'near Pareto front',color=c[3],fontsize=12)\n",
    "    ax1.text(0.1,0.8,'SG max $Q$',color=c[1],fontsize=12)\n",
    "    ax1.text(0.1,0.75,'SG with index '+str(SG_index),color=c[4],fontsize=12)\n",
    "    \n",
    "    bins_l=np.linspace(0.25, 1.50,num=30)\n",
    "    ax2.hist(df_SAC['bulk_modulus'], color=c[0], bins=bins_l, rwidth=0.9)\n",
    "    SG_max_Q=df_SAC.query(df_results['constraints'][0])\n",
    "    ax2.hist(SG_max_Q['bulk_modulus'], color=c[1], bins=bins_l, rwidth=0.9)\n",
    "    \n",
    "    ax3.hist(df_SAC['bulk_modulus'], color=c[0], bins=bins_l, rwidth=0.9)\n",
    "    SG_select_index=df_SAC.query(df_pr['constraints'][SG_index])\n",
    "    ax3.hist(SG_select_index['bulk_modulus'], color=c[4], bins=bins_l, rwidth=0.9)\n",
    "    \n",
    "    ax2.set_xlabel('$B_0$ (eV/$\\mathrm{\\AA}^3$)')\n",
    "    ax3.set_xlabel('$B_0$ (eV/$\\mathrm{\\AA}^3$)')\n",
    "    ax2.set_ylabel('Counts')\n",
    "    \n",
    "    ax2.text(0.4,47,'entire dataset',color=c[0],fontsize=12)\n",
    "    ax2.text(0.4,43,'SG max $Q$',color=c[1],fontsize=12)\n",
    "    ax3.text(0.4,47,'entire dataset',color=c[0],fontsize=12)\n",
    "    ax3.text(0.4,43,'SG with index '+str(SG_index),color=c[4],fontsize=12)\n",
    "    \n",
    "    print('Results for SG search '+id_job+':\\n',\n",
    "          '*constraints for SG that maximizes quality function (SG max Q) \\n',df_results['constraints'][0],\n",
    "          '\\n *constraints for SG of the Pareto region with index',SG_index,':\\n',df_pr['constraints'][SG_index])\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analysis of SG similarity and hierarchical clustering of SGD solutions of the Pareto region\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "for id_job in ['positive_mean_shift_SAC']:\n",
    "    fig, (ax1) = plt.subplots(1,1, constrained_layout=True, figsize=(4,4))\n",
    "    df_results=pd.read_csv('results_'+id_job+'.csv')\n",
    "    df_pr=pd.read_csv('pr_'+id_job+'.csv')\n",
    "    df_features=df_SAC.drop(['bulk_modulus'], axis=1)\n",
    "    \n",
    "    similarity_matrix=[]\n",
    "    for i in range(len(df_pr)):\n",
    "        Jaccard=[]\n",
    "        for j in range(len(df_pr)):\n",
    "            N1=df_features.query(df_pr.iloc[i]['constraints'])\n",
    "            N2=df_features.query(df_pr.iloc[j]['constraints'])\n",
    "            N1N2_combined= pd.concat([N1,N2])\n",
    "            overlap=(N1N2_combined.duplicated(keep='first').sum())/len(N1N2_combined.drop_duplicates())\n",
    "            Jaccard.append(overlap)\n",
    "        similarity_matrix.append(Jaccard)\n",
    "    ax1.set_ylabel('SG index')\n",
    "    ax1.set_xlabel('SG index')\n",
    "    im = ax1.imshow(similarity_matrix, vmin=0, vmax=1.0)\n",
    "    fig.colorbar(im, ax=ax1, label='Jaccard index', shrink=0.6, fraction=1.2)\n",
    "        \n",
    "    cmap=sns.clustermap(similarity_matrix,\n",
    "                   cmap=\"viridis\", \n",
    "                   figsize=(4,4),\n",
    "                   yticklabels=False,\n",
    "                   xticklabels=True,\n",
    "                   vmin=0,vmax=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the dataset containing 12,096 candidate perovskites and utilized for the screening of materials\n",
    "df_candidates=pd.read_csv('./data/candidates_for_screening.csv')\n",
    "#randomly selecting 50 candidate perovskites\n",
    "df_candidates.sample(n=50, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id='cJSD_AC'\n",
    "df_pf=pd.read_csv('pf_'+job_id+'.csv') \n",
    "df_results=pd.read_csv('results_'+job_id+'.csv')\n",
    "\n",
    "#applying the SG rules associated to the SG that maximizes Q\n",
    "df_candidates_selected_by_SG_max_quality=df_candidates.query(df_results['constraints'][0])\n",
    "print('The SG rules associated to SG max Q select',len(df_candidates_selected_by_SG_max_quality),'materials out of 12,096 candidates.')\n",
    "#randomly selecting 50 candidate perovskites from the materials that satisfy the SG rules\n",
    "df_candidates_selected_by_SG_max_quality.sample(n=50, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying the SG rules associated to the SGs with indicies 0, 1, 2, and 3 (with the highest utility-function values)\n",
    "print('SG rules associated to SG with index 0:\\n',df_pf.iloc[0]['constraints'])\n",
    "print('SG rules associated to SG with index 1:\\n',df_pf.iloc[1]['constraints'])\n",
    "print('SG rules associated to SG with index 2:\\n',df_pf.iloc[2]['constraints'])\n",
    "print('SG rules associated to SG with index 3:\\n', df_pf.iloc[3]['constraints'])\n",
    "df_candidates_selected_by_SGs_high_utility=df_candidates.query(df_pf.iloc[0]['constraints']+'&'+df_pf.iloc[1]['constraints']+'&'+df_pf.iloc[2]['constraints'])\n",
    "print('The SG rules associated to the SGs with high utility function select',len(df_candidates_selected_by_SGs_high_utility),'materials out of 12,096 candidates.')\n",
    "#randomly selecting 50 candidate perovskites from the materials that satisfy the SG rules\n",
    "df_candidates_selected_by_SGs_high_utility.sample(n=50, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comparison of bulk modulus for the materials indicated by random selection, by the SG rules identified\n",
    "#with the standard approach, and by the SG rules identified with the multi-objective approach\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "fig,(ax1,ax2,ax3,ax4) = plt.subplots(4,1, constrained_layout=True, figsize=(5,8))\n",
    "#loading the file with the DFT results\n",
    "df_DFT=pd.read_csv('./data/DFT_calculations.csv').set_index('material')            \n",
    "df_DFT_random=df_DFT.loc[df_DFT['label'] == 'random']\n",
    "df_DFT_SG_high_utility=df_DFT.loc[df_DFT['label'] == 'high utility']\n",
    "df_DFT_SG_max_quality=df_DFT.loc[df_DFT['label'] == 'max']\n",
    "\n",
    "bins_l=np.linspace(0.35, 1.70,num=30)\n",
    "ax1.hist(df_SAC['bulk_modulus'], color='k', bins=bins_l, rwidth=0.9)\n",
    "ax2.hist(df_DFT_random['bulk_modulus'], color='grey', bins=bins_l, rwidth=0.9)\n",
    "ax4.hist(df_DFT_SG_high_utility['bulk_modulus'], color='purple', bins=bins_l, rwidth=0.9)\n",
    "ax3.hist(df_DFT_SG_max_quality['bulk_modulus'], color='darkorange', bins=bins_l, rwidth=0.9)\n",
    "\n",
    "ax4.set_xlabel('$B_0$ (eV/$\\mathrm{\\AA}^3$)')\n",
    "ax2.set_ylabel('Counts')   \n",
    "ax1.set_title('training set')\n",
    "ax2.set_title('uniform sample of candidates')\n",
    "ax3.set_title('candidates selected by SG rules with maximum quality')\n",
    "ax4.set_title('candidates selected by SG rules with high exceptionality')\n",
    "lim=15\n",
    "ax1.set_ylim(0,60)\n",
    "ax2.set_ylim(0,lim)\n",
    "ax3.set_ylim(0,lim)\n",
    "ax4.set_ylim(0,lim)\n",
    "\n",
    "mean_data=df_SAC['bulk_modulus'].mean()\n",
    "max_data=df_SAC['bulk_modulus'].max()\n",
    "\n",
    "mean_random=df_DFT_random['bulk_modulus'].mean()\n",
    "max_random=df_DFT_random['bulk_modulus'].max()\n",
    "\n",
    "mean_SG_high_utility=df_DFT_SG_high_utility['bulk_modulus'].mean()\n",
    "max_SG_high_utility=df_DFT_SG_high_utility['bulk_modulus'].max()\n",
    "\n",
    "mean_SG_max_quality=df_DFT_SG_max_quality['bulk_modulus'].mean()\n",
    "max_SG_max_quality=df_DFT_SG_max_quality['bulk_modulus'].max()\n",
    "\n",
    "ax1.vlines(max_data,0,60,color='k',linestyle='dotted')\n",
    "ax1.vlines(mean_data,0,60,color='k',linestyle='dashed')\n",
    "\n",
    "ax2.vlines(max_random,0,15,color='k',linestyle='dotted')\n",
    "ax2.vlines(mean_random,0,15,color='k',linestyle='dashed')\n",
    "\n",
    "ax3.vlines(max_SG_max_quality,0,15,color='k',linestyle='dotted')\n",
    "ax3.vlines(mean_SG_max_quality,0,15,color='k',linestyle='dashed')\n",
    "\n",
    "ax4.vlines(max_SG_high_utility,0,15,color='k',linestyle='dotted')\n",
    "ax4.vlines(mean_SG_high_utility,0,15,color='k',linestyle='dashed')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
